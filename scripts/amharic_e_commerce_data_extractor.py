# -*- coding: utf-8 -*-
"""Amharic E-commerce Data Extractor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ULjlKf0PfF4XEo8hzVqrsrAFUKMTsjVA
"""

!pip install conllu

# -*- coding: utf-8 -*-
"""
Fine-Tune NER Model for Amharic Telegram Messages
This script fine-tunes the afroxmlr model on a labeled Amharic dataset in CoNLL format.
"""

# Install required libraries
!pip install transformers datasets torch seqeval
!pip install accelerate -U

import pandas as pd
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification
from seqeval.metrics import classification_report
import torch
from conllu import parse
import os



"""***#Set Up a Data Ingestion Pipeline***"""

#install telethon first
!pip install telethon
!pip install python-dotenv

"""***Fetch Data from Telegram Channels***"""

import nest_asyncio
nest_asyncio.apply()

import asyncio
from telethon import TelegramClient
from telethon.tl.functions.messages import GetHistoryRequest
import pandas as pd
from datetime import datetime

# Credentials
api_id = 25367099
api_hash = 'd7e39750bed65553fbceb37465d7689d'
phone = '+251966883822'

# Create client instance
client = TelegramClient('session_name', api_id, api_hash)

# Channel list
channels = [
    '@ZemenExpress',
    '@nevacomputer',
    '@ethio_brand_collection',
    '@meneshayeofficial',
    '@Leyueqa'
]

async def fetch_messages():
    await client.start(phone)

    all_data = []

    for ch in channels:
        try:
            entity = await client.get_entity(ch)

            messages = await client(GetHistoryRequest(
                peer=entity,
                limit=1000,  # Number of messages to fetch per channel
                offset_date=None,
                offset_id=0,
                max_id=0,
                min_id=0,
                add_offset=0,
                hash=0
            ))

            for msg in messages.messages:
                if hasattr(msg, 'message'):  # Check if message has text content
                    message_data = {
                        "channel": ch,
                        "date": msg.date,
                        "text": msg.message,
                        "views": getattr(msg, 'views', None),  # Some messages might not have views
                        "sender_id": getattr(msg, 'sender_id', None),
                        "message_id": msg.id,
                        "media_type": None
                    }

                    # Check for media
                    if hasattr(msg, 'media'):
                        if hasattr(msg.media, 'photo'):
                            message_data['media_type'] = 'photo'
                        elif hasattr(msg.media, 'document'):
                            message_data['media_type'] = 'document'

                    all_data.append(message_data)

        except Exception as e:
            print(f"Error in {ch}: {str(e)}")
            continue  # Continue with next channel if error occurs

    if all_data:
        df = pd.DataFrame(all_data)
        df.to_csv("telegram_scraped_data.csv", index=False)
        print(f"Successfully saved {len(df)} messages to telegram_scraped_data.csv")
        print(df.head())
    else:
        print("No messages were fetched. Check your channel list and connection.")

# Run the async task
await fetch_messages()

import pandas as pd

# Load and display the CSV as a DataFrame
df = pd.read_csv('telegram_scraped_data.csv')

# Display the first 5 rows (or change to df.head(10) for 10 rows, etc.)
df.head(1000)

"""Setup"""

!pip install etnltk

"""***Amharic text preprocessing with Amharic document***"""

from etnltk import Amharic
import pandas as pd

# Load the CSV
df = pd.read_csv('telegram_scraped_data.csv')

# Use the correct column name (e.g., 'text' instead of 'Text')
df['Cleaned_Text'] = df['text'].apply(lambda x: str(Amharic(str(x))) if pd.notnull(x) else '')

# Show cleaned examples
df[['text', 'Cleaned_Text']].head(1000)

"""***Clean and Structure the Data***"""

# 1. Install etnltk
!pip install -U etnltk

# 2. Imports
import pandas as pd
import re
from etnltk.lang.am import clean_amharic

# 3. Define custom cleaning functions
def remove_emojis(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_digits(text):
    return re.sub(r'\d+', '', text)

def remove_english_chars(text):
    return re.sub(r'[A-Za-z]', '', text)

def remove_ethiopic_punctuation(text):
    ethiopic_punct = "፡።፣፤፥፦፧፨"
    return re.sub(f"[{ethiopic_punct}]", '', text)

# 4. Load the CSV
df = pd.read_csv('telegram_scraped_data.csv')

# 5. Normalize column names
df.columns = df.columns.str.strip().str.lower()

# 6. Confirm the text column exists
if 'text' not in df.columns:
    raise KeyError("Expected a column named 'text' in the CSV.")

# 7. Define cleaning pipeline
custom_pipeline = [
    remove_emojis,
    remove_digits,
    remove_english_chars,
    remove_ethiopic_punctuation
]

# 8. Apply cleaning
df['cleaned_text'] = df['text'].apply(
    lambda x: clean_amharic(str(x), pipeline=custom_pipeline) if pd.notnull(x) else ''
)

# 9. Preview
df[['text', 'cleaned_text']].head(100)

"""Tokenization"""

from etnltk import Amharic

# Helper function to process non-empty cleaned text
def extract_sentences(text):
    if pd.notnull(text) and text.strip() != '':
        return Amharic(text).sentences
    return []

# Apply to each row safely
df['sentences'] = df['cleaned_text'].apply(extract_sentences)

# Preview
df[['cleaned_text', 'sentences']].head(1000)

"""Tokenization"""

from etnltk.tokenize.am import word_tokenize

# Tokenize each cleaned_text entry (if not empty)
df['tokens'] = df['cleaned_text'].apply(
    lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
)

# Preview some tokens
df[['cleaned_text', 'tokens']].head(1000)

df['tokens'] = df['text'].apply(
    lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
)
print(df[['text', 'tokens']].head(1000))

# Example: Clean raw text (remove unwanted characters, normalize, etc.)
df['cleaned_text'] = df['text'].str.replace(r'[^\u1200-\u137F\s]', '', regex=True)  # Keep Ge'ez script and spaces

import pandas as pd
from etnltk.tokenize.am import word_tokenize

# Create subset (e.g., first 100 rows)
subset_df = df.head(100)  # Adjust as needed

# Save tokens to a text file for manual annotation
with open('amharic_tokens_for_labeling.txt', 'w', encoding='utf-8') as f:
    for idx, row in subset_df.iterrows():
        if row['tokens']:  # Skip empty token lists
            f.write(f"# text: {row['text']}\n")  # Use correct column name
            for token in row['tokens']:
                f.write(f"{token}\t_\t_\n")  # Placeholder for POS, NER
            f.write("\n")

# Add placeholder tags (replace with actual annotations if available)
subset_df['ner_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['O'] * len(tokens) if tokens else []
)
subset_df['pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

# Save to CoNLL
def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        for idx, row in df.iterrows():
            if not row[token_col]:  # Skip empty token lists
                continue
            f.write(f"# text: {row['text']}\n")  # Use correct column name
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")

save_to_conll(subset_df, 'amharic_ner.conll')

import pandas as pd
from etnltk.tokenize.am import word_tokenize
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# Load xlm-roberta-base model and tokenizer for NER
model_name = "masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# Assuming df is your DataFrame with a 'text' column
subset_df = df.head(100).copy()  # Create a copy to avoid SettingWithCopyWarning

def perform_ner(text):
    # Handle non-string or NaN inputs
    if not isinstance(text, str) or pd.isna(text):
        return [], [], []

    # Tokenize the text using etnltk
    tokens = word_tokenize(text)
    if not tokens:
        return [], [], []

    # Perform NER using xlm-roberta-base
    ner_results = ner_pipeline(text)

    # Initialize ner_tags and pos_tags
    ner_tags = ['O'] * len(tokens)
    pos_tags = ['_'] * len(tokens)  # POS tags remain as placeholder

    # Map NER results to tokens
    current_token_idx = 0
    for entity in ner_results:
        entity_text = entity['word']
        entity_label = entity['entity_group']

        # Find matching tokens for the entity
        for i in range(current_token_idx, len(tokens)):
            if entity_text in tokens[i] or tokens[i] in entity_text:
                ner_tags[i] = entity_label
                current_token_idx = i + 1
                break

    return tokens, ner_tags, pos_tags

# Apply NER to the DataFrame
subset_df[['tokens', 'ner_tags', 'pos_tags']] = pd.DataFrame(
    subset_df['text'].apply(perform_ner).tolist(),
    index=subset_df.index
)

def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        for idx, row in df.iterrows():
            if not row[token_col]:  # Skip empty token lists
                continue
            f.write(f"# text: {row['text']}\n")
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")

# Save to CoNLL format
save_to_conll(subset_df, 'amharic_ner.conll')

print(subset_df.columns)
print(subset_df[['tokens', 'ner_tags']].head(1000))

# Check for length mismatches
for idx, row in subset_df.iterrows():
    if row['tokens'] and len(row['tokens']) != len(row['ner_tags']):
        print(f"Row {idx}: Token-NER mismatch. Tokens: {len(row['tokens'])}, NER tags: {len(row['ner_tags'])}")
    if row['tokens'] and len(row['tokens']) != len(row['pos_tags']):
        print(f"Row {idx}: Token-POS mismatch. Tokens: {len(row['tokens'])}, POS tags: {len(row['pos_tags'])}")

subset_df['ner_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['O'] * len(tokens) if tokens else []
)
subset_df['pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

save_to_conll(subset_df, 'amharic_ner.conll')

with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
    print("\n".join(f.readlines()[:1000]))

import pandas as pd
from etnltk.tokenize.am import word_tokenize
import re

# Verify DataFrame exists
if 'df' not in globals():
    raise NameError("DataFrame 'df' is not defined. Load or create it first.")
if 'text' not in df.columns:
    raise KeyError("Column 'text' not found. Available columns: " + str(df.columns.tolist()))

# Create a copy of subset to avoid SettingWithCopyWarning
subset_df = df.head(100).copy()

# Tokenize if not already done
if 'tokens' not in subset_df.columns:
    subset_df['tokens'] = subset_df['text'].apply(
        lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
    )

# Filter out rows with non-Amharic or problematic text
def is_valid_amharic_text(text):
    if pd.isna(text) or not text.strip():
        return False
    # Keep text with at least some Amharic characters (Ge'ez script: U+1200–U+137F)
    return bool(re.search(r'[\u1200-\u137F]', text))

subset_df = subset_df[subset_df['text'].apply(is_valid_amharic_text)]

# Add placeholder tags
subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['O'] * len(tokens) if tokens else []
)
subset_df.loc[:, 'pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

# Debug: Check for empty tokens
empty_token_rows = subset_df[subset_df['tokens'].apply(len) == 0]
if not empty_token_rows.empty:
    print(f"Warning: {len(empty_token_rows)} rows have empty tokens:")
    print(empty_token_rows[['text', 'tokens']].head())

# Save to CoNLL
def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        sentence_count = 0
        for idx, row in df.iterrows():
            if not row[token_col]:
                continue
            # Only include text with Amharic content in comments
            if not is_valid_amharic_text(row['text']):
                continue
            f.write(f"# text: {row['text']}\n")
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            if len(tokens) != len(ner_tags) or len(tokens) != len(pos_tags):
                print(f"Row {idx}: Mismatch. Tokens: {len(tokens)}, NER: {len(ner_tags)}, POS: {len(pos_tags)}")
                continue

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                # Skip tokens that are purely emojis or non-text
                if not re.match(r'^[\u1200-\u137F0-9a-zA-Z]+$', token):
                    continue
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")
            sentence_count += 1
        print(f"Wrote {sentence_count} sentences to {output_file}")

save_to_conll(subset_df, 'amharic_ner.conll')

# Validate
from conllu import parse
try:
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        data = f.read()
    sentences = parse(data)
    print(f"Parsed {len(sentences)} sentences")
except Exception as e:
    print(f"Parsing failed: {e}")
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        print("\nFirst 20 lines of file:")
        print("\n".join(f.readlines()[:20]))

def rule_based_ner(tokens):
    ner_tags = []
    for i, token in enumerate(tokens):
        if token in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'}:
            ner_tags.append('B-LOC' if i == 0 or tokens[i-1] not in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'} else 'I-LOC')
        elif token.startswith('@'):
            ner_tags.append('B-ORG')
        elif token == 'ብር':
            ner_tags.append('I-PRICE' if i > 0 and tokens[i-1].isdigit() else 'O')
        elif token.isdigit():
            ner_tags.append('B-PRICE')
        else:
            ner_tags.append('O')
    return ner_tags

subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(rule_based_ner)
save_to_conll(subset_df, 'amharic_ner.conll')

import pandas as pd
from etnltk.tokenize.am import word_tokenize
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import re
from conllu import parse

# Verify DataFrame exists
if 'df' not in globals():
    raise NameError("DataFrame 'df' is not defined. Load or create it first.")
if 'text' not in df.columns:
    raise KeyError("Column 'text' not found. Available columns: " + str(df.columns.tolist()))

# Create a copy of subset to avoid SettingWithCopyWarning
subset_df = df.head(100).copy()

# Tokenize if not already done
if 'tokens' not in subset_df.columns:
    subset_df['tokens'] = subset_df['text'].apply(
        lambda x: word_tokenize(x) if pd.notnull(x) and x.strip() != '' else []
    )

# Filter out rows with non-Amharic or problematic text
def is_valid_amharic_text(text):
    if pd.isna(text) or not text.strip():
        return False
    return bool(re.search(r'[\u1200-\u137F]', text))  # Ge'ez script

subset_df = subset_df[subset_df['text'].apply(is_valid_amharic_text)]

# Debug: Check for empty tokens
empty_token_rows = subset_df[subset_df['tokens'].apply(len) == 0]
if not empty_token_rows.empty:
    print(f"Warning: {len(empty_token_rows)} rows have empty tokens:")
    print(empty_token_rows[['text', 'tokens']].head())

# Initialize NER pipeline
try:
    tokenizer = AutoTokenizer.from_pretrained("Davlan/bert-base-multilingual-cased-ner-hrl")
    model = AutoModelForTokenClassification.from_pretrained("Davlan/bert-base-multilingual-cased-ner-hrl")
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")
except OSError as e:
    print(f"Error loading model: {e}")
    print("Ensure you have internet access and the correct model identifier.")
    raise

# Predict NER tags with token alignment
def predict_ner(tokens):
    if not tokens:
        return []
    text = " ".join(tokens)
    try:
        predictions = ner_pipeline(text)
    except Exception as e:
        print(f"NER pipeline failed for text: {text}\nError: {e}")
        return ['O'] * len(tokens)

    # Align predictions with tokens
    ner_tags = ['O'] * len(tokens)
    token_idx = 0
    current_offset = 0
    for pred in predictions:
        entity = pred['entity_group']
        start = pred['start']
        end = pred['end']
        # Find which token(s) the entity spans
        token_text = "".join(tokens)  # Concatenate without spaces for offset calculation
        while token_idx < len(tokens):
            token = tokens[token_idx]
            token_start = current_offset
            token_end = current_offset + len(token)
            if start >= token_end:
                current_offset += len(token) + 1  # +1 for space
                token_idx += 1
                continue
            if end <= token_start:
                break
            # Assign entity to the token (simplified: assign to first token of entity)
            ner_tags[token_idx] = entity
            break
    return ner_tags

# Apply NER predictions
subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(predict_ner)
subset_df.loc[:, 'pos_tags'] = subset_df['tokens'].apply(
    lambda tokens: ['_'] * len(tokens) if tokens else []
)

# Save to CoNLL
def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        sentence_count = 0
        for idx, row in df.iterrows():
            if not row[token_col]:
                continue
            if not is_valid_amharic_text(row['text']):
                continue
            f.write(f"# text: {row['text']}\n")
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)

            if len(tokens) != len(ner_tags) or len(tokens) != len(pos_tags):
                print(f"Row {idx}: Mismatch. Tokens: {len(tokens)}, NER: {len(ner_tags)}, POS: {len(pos_tags)}")
                continue

            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                if not re.match(r'^[\u1200-\u137F0-9a-zA-Z]+$', token):
                    continue
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")
            sentence_count += 1
        print(f"Wrote {sentence_count} sentences to {output_file}")

save_to_conll(subset_df, 'amharic_ner.conll')

# Validate
try:
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        data = f.read()
    sentences = parse(data)
    print(f"Parsed {len(sentences)} sentences")
except Exception as e:
    print(f"Parsing failed: {e}")
    with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
        print("\nFirst 20 lines of file:")
        print("\n".join(f.readlines()[:100]))

def rule_based_ner(tokens):
    ner_tags = []
    for i, token in enumerate(tokens):
        if token in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'}:
            ner_tags.append('B-LOC' if i == 0 or tokens[i-1] not in {'መገናኛ', 'መሰረት', 'ደፋር', 'ሞል'} else 'I-LOC')
        elif token.startswith('@'):
            ner_tags.append('B-ORG')
        elif token == 'ብር':
            ner_tags.append('I-PRICE' if i > 0 and tokens[i-1].isdigit() else 'O')
        elif token.isdigit():
            ner_tags.append('B-PRICE')
        else:
            ner_tags.append('O')
    return ner_tags

subset_df.loc[:, 'ner_tags'] = subset_df['tokens'].apply(rule_based_ner)
save_to_conll(subset_df, 'amharic_ner.conll')

with open('amharic_ner.conll', 'r', encoding='utf-8') as f:
    print("\n".join(f.readlines()[:20]))


"""***Fine Tuning***"""

import os
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Step 1: Load the CoNLL dataset
def load_conll(file_path):
    tokens_list = []
    ner_tags_list = []
    current_tokens = []
    current_tags = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                if current_tokens and current_tags:
                    tokens_list.append(current_tokens)
                    ner_tags_list.append(current_tags)
                    current_tokens = []
                    current_tags = []
                continue
            # Split by tab or multiple spaces
            fields = line.split('\t') if '\t' in line else line.split()
            if len(fields) >= 2:
                current_tokens.append(fields[0])
                current_tags.append(fields[-1])  # Assume last field is NER tag
            else:
                print(f"Skipping malformed line: {line}")

    # Append the last sentence if exists
    if current_tokens and current_tags:
        tokens_list.append(current_tokens)
        ner_tags_list.append(current_tags)

    return pd.DataFrame({'tokens': tokens_list, 'ner_tags': ner_tags_list})

# Load the dataset
conll_file = 'amharic_ner.conll'
if not os.path.exists(conll_file):
    raise FileNotFoundError(f"{conll_file} not found. Please upload the file.")
df = load_conll(conll_file)
print(f"Loaded {len(df)} sentences from {conll_file}")

# Step 2: Convert to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Step 3: Split into train and validation sets
train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# Step 4: Load tokenizer and model
model_name = "Davlan/afro-xlmr-base"  # afro-xlmr model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=len(set(tag for tags in df['ner_tags'] for tag in tags))
)

# Step 5: Define label mapping
unique_labels = sorted(set(tag for tags in df['ner_tags'] for tag in tags))
label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}
model.config.label2id = label2id
model.config.id2label = id2label

import pandas as pd
import os
from etnltk.tokenize.am import word_tokenize
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, Trainer, TrainingArguments
from datasets import Dataset
import numpy as np

# Disable W&B if not needed
os.environ['WANDB_DISABLED'] = 'true'  # Uncomment if you don't want W&B logging
# Or set W&B API key if using W&B
# os.environ['WANDB_API_KEY'] = 'your-40-character-api-key'

# Load xlm-roberta-base model and tokenizer
model_name = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=9)  # Adjust num_labels based on your NER tags
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# Assuming df is your DataFrame
subset_df = df.head(100).copy()

# Check if 'tokens' column exists; otherwise, tokenize from a text column
text_column = 'sentence'  # Replace with the actual column name (e.g., 'sentence', 'content')
if 'tokens' not in subset_df.columns and text_column in subset_df.columns:
    def perform_ner(row):
        text = row[text_column]
        if not isinstance(text, str) or pd.isna(text):
            return [], [], []

        tokens = word_tokenize(text)
        if not tokens:
            return [], [], []

        ner_results = ner_pipeline(text)
        ner_tags = ['O'] * len(tokens)
        pos_tags = ['_'] * len(tokens)

        current_token_idx = 0
        for entity in ner_results:
            entity_text = entity['word']
            entity_label = entity['entity_group']
            for i in range(current_token_idx, len(tokens)):
                if entity_text in tokens[i] or tokens[i] in entity_text:
                    ner_tags[i] = entity_label
                    current_token_idx = i + 1
                    break

        return tokens, ner_tags, pos_tags

    # Apply NER to the DataFrame
    subset_df[['tokens', 'ner_tags', 'pos_tags']] = pd.DataFrame(
        subset_df.apply(perform_ner, axis=1).tolist(),
        index=subset_df.index
    )
elif 'tokens' in subset_df.columns:
    def perform_ner_from_tokens(row):
        tokens = row['tokens']
        if not tokens or not isinstance(tokens, list):
            return [], [], []

        # Join tokens to create text for NER pipeline
        text = ' '.join(tokens)
        ner_results = ner_pipeline(text)
        ner_tags = ['O'] * len(tokens)
        pos_tags = ['_'] * len(tokens)

        current_token_idx = 0
        for entity in ner_results:
            entity_text = entity['word']
            entity_label = entity['entity_group']
            for i in range(current_token_idx, len(tokens)):
                if entity_text in tokens[i] or tokens[i] in entity_text:
                    ner_tags[i] = entity_label
                    current_token_idx = i + 1
                    break

        return tokens, ner_tags, pos_tags

    # Apply NER to the DataFrame
    subset_df[['tokens', 'ner_tags', 'pos_tags']] = pd.DataFrame(
        subset_df.apply(perform_ner_from_tokens, axis=1).tolist(),
        index=subset_df.index
    )
else:
    raise KeyError("DataFrame must have either a 'tokens' column or a text column (e.g., 'sentence')")

def save_to_conll(df, output_file, token_col='tokens', ner_col='ner_tags', pos_col='pos_tags'):
    with open(output_file, 'w', encoding='utf-8') as f:
        for idx, row in df.iterrows():
            if not row[token_col]:
                continue
            # Use the text_column or joined tokens for the header
            text = row[text_column] if text_column in df.columns else ' '.join(row[token_col])
            f.write(f"# text: {text}\n")
            tokens = row[token_col]
            ner_tags = row[ner_col] if ner_col in df.columns else ['O'] * len(tokens)
            pos_tags = row[pos_col] if pos_col in df.columns else ['_'] * len(tokens)
            for token, pos, ner in zip(tokens, pos_tags, ner_tags):
                f.write(f"{token}\t{pos}\t{ner}\n")
            f.write("\n")

# Save to CoNLL format
save_to_conll(subset_df, 'amharic_ner.conll')

# Prepare dataset for fine-tuning
def prepare_dataset(df):
    dataset = []
    for _, row in df.iterrows():
        if not row['tokens']:
            continue
        dataset.append({
            'tokens': row['tokens'],
            'ner_tags': row['ner_tags']
        })
    return Dataset.from_list(dataset)

# Convert to Hugging Face Dataset
hf_dataset = prepare_dataset(subset_df)

# Define label mappings (adjust based on your NER tags)
label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']  # Example labels
label_to_id = {label: i for i, label in enumerate(label_list)}

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True, padding=True)
    labels = []
    for i, label in enumerate(examples['ner_tags']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label_to_id.get(label[word_idx], 0))
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs['labels'] = labels
    return tokenized_inputs

# Tokenize dataset
tokenized_dataset = hf_dataset.map(tokenize_and_align_labels, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',  # Changed from evaluation_strategy to eval_strategy
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    report_to=None,  # Disable W&B and other logging
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,  # Use a separate eval set if available
    tokenizer=tokenizer,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained('./fine_tuned_xlm_roberta')
tokenizer.save_pretrained('./fine_tuned_xlm_roberta')

# Load fine-tuned model for inference
ner_pipeline = pipeline("ner", model='./fine_tuned_xlm_roberta', tokenizer='./fine_tuned_xlm_roberta', aggregation_strategy="simple")

"""***Model Comparison & Selection***"""

!pip install evaluate

import pandas as pd
import os
import time
import numpy as np
from etnltk.tokenize.am import word_tokenize
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.model_selection import train_test_split
from evaluate import load
import warnings
warnings.filterwarnings("ignore")

# Disable W&B to avoid API key issues
os.environ['WANDB_DISABLED'] = 'true'

# Define models to compare
models = [
    {"name": "xlm-roberta-base", "num_labels": 9},
    {"name": "distilbert-base-multilingual-cased", "num_labels": 9},
    {"name": "bert-base-multilingual-cased", "num_labels": 9},
]

# Define NER labels (adjust based on your dataset)
label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
label_to_id = {label: i for i, label in enumerate(label_list)}
id_to_label = {i: label for i, label in enumerate(label_list)}

# Assuming df is your DataFrame
print("Available columns in DataFrame:", df.columns.tolist())

# Set text_column to the actual column name containing text (update after checking df.columns)
text_column = 'content'  # Replace with actual column name (e.g., 'text', 'document')

# Check if text_column or tokens column exists
if 'tokens' not in df.columns and text_column not in df.columns:
    raise KeyError(f"DataFrame must have either a 'tokens' column or a '{text_column}' column. Available columns: {df.columns.tolist()}")

# Clean DataFrame
if text_column in df.columns:
    df = df.dropna(subset=[text_column])
    df = df[df[text_column].str.strip() != '']

# Split into train and validation sets
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
train_df = train_df.head(100)  # Limit for testing; remove for full dataset
val_df = val_df.head(20)  # Limit for testing; remove for full dataset

def prepare_ner_data(row):
    if 'tokens' in row and isinstance(row['tokens'], list) and row['tokens']:
        tokens = row['tokens']
        # Use existing NER tags if available, else placeholder
        ner_tags = row['ner_tags'] if 'ner_tags' in row and isinstance(row['ner_tags'], list) else ['O'] * len(tokens)
        pos_tags = ['_'] * len(tokens)
    else:
        text = row[text_column] if text_column in row else ''
        if not isinstance(text, str) or pd.isna(text) or not text.strip():
            return [], [], []

        tokens = word_tokenize(text)
        if not tokens:
            return [], [], []

        ner_tags = ['O'] * len(tokens)  # Placeholder; replace with actual labels if available
        pos_tags = ['_'] * len(tokens)

    return tokens, ner_tags, pos_tags

# Apply NER preparation to train and validation DataFrames
for df_subset in [train_df, val_df]:
    df_subset[['tokens', 'ner_tags', 'pos_tags']] = pd.DataFrame(
        df_subset.apply(prepare_ner_data, axis=1).tolist(),
        index=df_subset.index
    )

def prepare_dataset(df):
    dataset = []
    for _, row in df.iterrows():
        if not row['tokens']:
            continue
        dataset.append({
            'tokens': row['tokens'],
            'ner_tags': row['ner_tags']
        })
    return Dataset.from_list(dataset)

# Convert to Hugging Face Datasets
train_dataset = prepare_dataset(train_df)
val_dataset = prepare_dataset(val_df)

def tokenize_and_align_labels(examples, tokenizer):
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True, padding=True)
    labels = []
    for i, label in enumerate(examples['ner_tags']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label_to_id.get(label[word_idx], 0))
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs['labels'] = labels
    return tokenized_inputs

# Load evaluation metric
try:
    metric = load("seqeval")
except ModuleNotFoundError:
    raise ModuleNotFoundError("Please install the 'evaluate' library: pip install evaluate")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [id_to_label[p] for p, l in zip(pred, label) if l != -100]
        for pred, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

# Model comparison results
comparison_results = []

for model_config in models:
    model_name = model_config["name"]
    num_labels = model_config["num_labels"]
    print(f"\nFine-tuning {model_name}...")

    # Load tokenizer and model
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)
    except Exception as e:
        print(f"Error loading {model_name}: {e}")
        continue

    # Tokenize datasets
    tokenized_train = train_dataset.map(
        lambda x: tokenize_and_align_labels(x, tokenizer), batched=True
    )
    tokenized_val = val_dataset.map(
        lambda x: tokenize_and_align_labels(x, tokenizer), batched=True
    )

    # Define training arguments
    training_args = TrainingArguments(
        output_dir=f'./results_{model_name.replace("/", "_")}',
        eval_strategy='epoch',
        learning_rate=2e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=3,
        weight_decay=0.01,
        report_to=None,
        logging_steps=10,
    )

    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # Fine-tune the model
    try:
        trainer.train()
    except Exception as e:
        print(f"Error during training {model_name}: {e}")
        continue

    # Evaluate on validation set
    eval_results = trainer.evaluate()

    # Measure inference speed
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")
    sample_text = train_df[text_column].iloc[0] if text_column in train_df.columns else ' '.join(train_df['tokens'].iloc[0])
    start_time = time.time()
    for _ in range(10):  # Run 10 times for average
        ner_pipeline(sample_text)
    inference_time = (time.time() - start_time) / 10  # Average time per inference

    # Store results
    comparison_results.append({
        "model": model_name,
        "f1": eval_results.get("eval_f1", 0.0),
        "precision": eval_results.get("eval_precision", 0.0),
        "recall": eval_results.get("eval_recall", 0.0),
        "inference_time (s)": inference_time,
    })

    # Save model
    model.save_pretrained(f'./fine_tuned_{model_name.replace("/", "_")}')
    tokenizer.save_pretrained(f'./fine_tuned_{model_name.replace("/", "_")}')

# Create comparison DataFrame
comparison_df = pd.DataFrame(comparison_results)
print("\nModel Comparison Results:")
print(comparison_df)

# Select best model based on F1-score and inference time
if not comparison_df.empty:
    best_model = comparison_df.loc[comparison_df['f1'].idxmax()]
    print(f"\nBest Model: {best_model['model']}")
    print(f"F1-Score: {best_model['f1']:.4f}")
    print(f"Inference Time: {best_model['inference_time (s)']:.4f} seconds")
else:
    print("No models were successfully evaluated.")

# Save comparison results to CSV
comparison_df.to_csv('model_comparison_results.csv', index=False)

"""***Model Interpretability***"""

!pip install lime

import pandas as pd
import numpy as np
import os
from etnltk.tokenize.am import word_tokenize
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from datasets import Dataset
import shap
try:
    import lime
    from lime.lime_text import LimeTextExplainer
except ModuleNotFoundError:
    raise ModuleNotFoundError("Please install the 'lime' library: pip install lime")
import torch
import warnings
from sklearn.model_selection import train_test_split
warnings.filterwarnings("ignore")

# Define model and tokenizer path (resolve local path)
model_path = os.path.abspath("fine_tuned_xlm-roberta-base")  # Use absolute path
fallback_model = "xlm-roberta-base"  # Fallback if local model is missing

# Load model and tokenizer
try:
    if not os.path.exists(model_path):
        print(f"Local model directory '{model_path}' not found. Using Fallback model '{fallback_model}'.")
        tokenizer = AutoTokenizer.from_pretrained(fallback_model)
        model = AutoModelForTokenClassification.from_pretrained(fallback_model, num_labels=len(label_list))
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForTokenClassification.from_pretrained(model_path)
except Exception as e:
    print(f"Error loading model from {model_path}: {e}. Falling back to '{fallback_model}'.")
    tokenizer = AutoTokenizer.from_pretrained(fallback_model)
    model = AutoModelForTokenClassification.from_pretrained(fallback_model, num_labels=len(label_list))

# Define NER labels (must match those used in fine-tuning)
label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
id_to_label = {i: label for i, label in enumerate(label_list)}

# Assuming df is your DataFrame
print("Available columns in DataFrame:", df.columns.tolist())
text_column = 'content'  # Replace with actual column name (e.g., 'text', 'document')

# Check if text_column or tokens column exists
if 'tokens' not in df.columns and text_column not in df.columns:
    raise KeyError(f"DataFrame must have either a 'tokens' column or a '{text_column}' column. Available columns: {df.columns.tolist()}")

# Clean DataFrame
if text_column in df.columns:
    df = df.dropna(subset=[text_column])
    df = df[df[text_column].str.strip() != '']

# Split into train and validation sets (for analysis)
_, val_df = train_test_split(df, test_size=0.2, random_state=42)
val_df = val_df.head(10)  # Limit for analysis; adjust as needed

# Prepare data
def prepare_ner_data(row):
    if 'tokens' in row and isinstance(row['tokens'], list) and row['tokens']:
        tokens = row['tokens']
        ner_tags = row['ner_tags'] if 'ner_tags' in row and isinstance(row['ner_tags'], list) else ['O'] * len(tokens)
    else:
        text = row[text_column] if text_column in row else ''
        if not isinstance(text, str) or pd.isna(text) or not text.strip():
            return [], []
        tokens = word_tokenize(text)
        ner_tags = ['O'] * len(tokens) if 'ner_tags' not in row else row['ner_tags']
    return tokens, ner_tags

# Apply to validation DataFrame
val_df[['tokens', 'ner_tags']] = pd.DataFrame(
    val_df.apply(prepare_ner_data, axis=1).tolist(),
    index=val_df.index
)

# Initialize NER pipeline
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# SHAP Explainer
def predict_proba(texts):
    """Predict NER probabilities for SHAP."""
    if isinstance(texts, str):
        texts = [texts]
    predictions = []
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1).squeeze().numpy()
        # Aggregate token probabilities to word-level
        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        word_probs = []
        current_word = ""
        current_prob = np.zeros(len(label_list))
        for token, prob in zip(tokens, probs):
            if token.startswith("##"):
                current_word += token[2:]
                current_prob += prob
            else:
                if current_word:
                    word_probs.append(current_prob / max(1, len(current_word.split())))
                current_word = token
                current_prob = prob
        if current_word:
            word_probs.append(current_prob / max(1, len(current_word.split())))
        predictions.append(np.array(word_probs))
    return np.array(predictions)

try:
    shap_explainer = shap.KernelExplainer(predict_proba, [val_df[text_column].iloc[0]])
except Exception as e:
    print(f"SHAP initialization failed: {e}")

# LIME Explainer
class NERLimeExplainer:
    def __init__(self, pipeline, label_list):
        self.pipeline = pipeline
        self.label_list = label_list

    def predict(self, texts):
        """Predict NER labels for LIME."""
        if isinstance(texts, str):
            texts = [texts]
        probs = []
        for text in texts:
            ner_results = self.pipeline(text)
            tokens = word_tokenize(text)
            token_probs = np.zeros((len(tokens), len(self.label_list)))
            for result in ner_results:
                entity_text = result['word']
                label = result['entity_group']
                if label in self.label_list:
                    label_idx = self.label_list.index(label)
                    for i, token in enumerate(tokens):
                        if entity_text in token or token in entity_text:
                            token_probs[i, label_idx] = result['score']
            for i in range(len(tokens)):
                if token_probs[i].sum() == 0:
                    token_probs[i, self.label_list.index('O')] = 1.0
                else:
                    token_probs[i] /= token_probs[i].sum()
            probs.append(token_probs)
        return np.array(probs)

lime_explainer = LimeTextExplainer(class_names=label_list)

# Analyze difficult cases
difficult_cases = []
interpretability_report = []

for idx, row in val_df.iterrows():
    text = row[text_column] if text_column in row else ' '.join(row['tokens'])
    true_labels = row['ner_tags'] if 'ner_tags' in row else ['O'] * len(row['tokens'])
    tokens = row['tokens']

    # Run NER pipeline
    ner_results = ner_pipeline(text)
    pred_labels = ['O'] * len(tokens)
    for result in ner_results:
        entity_text = result['word']
        label = result['entity_group']
        for i, token in enumerate(tokens):
            if entity_text in token or token in entity_text:
                pred_labels[i] = label
                break

    # Check for mismatches (difficult cases)
    mismatches = [(t, true, pred) for t, true, pred in zip(tokens, true_labels, pred_labels) if true != pred]
    if mismatches:
        difficult_cases.append({
            "text": text,
            "tokens": tokens,
            "true_labels": true_labels,
            "pred_labels": pred_labels,
            "mismatches": mismatches
        })

    # SHAP explanation
    try:
        shap_values = shap_explainer.shap_values(text, nsamples=50)
        shap_explanation = []
        for i, token in enumerate(tokens):
            if i < len(shap_values[0]):
                top_label_idx = np.argmax(shap_values[0][i])
                top_label = label_list[top_label_idx]
                shap_explanation.append(f"Token: {token}, Top Label: {top_label}, SHAP Contribution: {shap_values[0][i][top_label_idx]:.4f}")
    except Exception as e:
        shap_explanation = [f"SHAP failed for text: {e}"]

    # LIME explanation
    try:
        lime_exp = lime_explainer.explain_instance(
            text,
            lambda x: NERLimeExplainer(ner_pipeline, label_list).predict(x),
            num_features=6,
            num_samples=100
        )
        lime_explanation = lime_exp.as_list()
    except Exception as e:
        lime_explanation = [(f"LIME failed: {e}", 0.0)]

    # Add to report
    interpretability_report.append({
        "text": text,
        "shap_explanation": shap_explanation,
        "lime_explanation": lime_explanation,
        "mismatches": mismatches
    })

# Generate report
report_lines = []
report_lines.append("# NER Model Interpretability Report\n")
report_lines.append("## Model: XLM-RoBERTa-base (fine-tuned)\n")
report_lines.append("## Date: June 23, 2025\n")
report_lines.append("### Summary\n")
report_lines.append("This report analyzes the interpretability of the fine-tuned XLM-RoBERTa-base model for Amharic NER using SHAP and LIME.\n")

for i, case in enumerate(interpretability_report):
    report_lines.append(f"\n### Case {i+1}: {case['text'][:100]}...\n")
    report_lines.append("#### SHAP Explanations\n")
    for exp in case['shap_explanation']:
        report_lines.append(f"- {exp}\n")
    report_lines.append("#### LIME Explanations\n")
    for feature, weight in case['lime_explanation']:
        report_lines.append(f"- Feature: {feature}, Weight: {weight:.4f}\n")
    if case['mismatches']:
        report_lines.append("#### Mismatches (Difficult Cases)\n")
        for token, true, pred in case['mismatches']:
            report_lines.append(f"- Token: {token}, True: {true}, Predicted: {pred}\n")

report_lines.append("\n## Analysis of Difficult Cases\n")
if difficult_cases:
    report_lines.append("- **Ambiguous Text**: Tokens misclassified due to context ambiguity (e.g., names that could be persons or locations).\n")
    report_lines.append("- **Overlapping Entities**: Challenges with multi-word entities or nested entities in Amharic.\n")
    report_lines.append("- **Suggestions for Improvement**:\n")
    report_lines.append("  - Fine-tune on a larger, high-quality Amharic NER dataset (e.g., MasakhaNER).\n")
    report_lines.append("  - Incorporate context-aware features (e.g., sentence embeddings).\n")
    report_lines.append("  - Use ensemble methods to combine XLM-RoBERTa with lighter models like DistilBERT.\n")
else:
    report_lines.append("- No mismatches found in the analyzed samples.\n")

# Save report
with open("ner_interpretability_report.md", "w", encoding="utf-8") as f:
    f.write("".join(report_lines))

print("Interpretability report saved to 'ner_interpretability_report.md'")

"""***FinTech Vendor Scorecard for Micro-Lending***"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
from etnltk.tokenize.am import word_tokenize
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import re
import warnings
warnings.filterwarnings("ignore")

# Sample DataFrame (use if raw data is missing)
sample_data = {
    'vendor': ['VendorA', 'VendorA', 'VendorB', 'VendorB'],
    'content': [
        'New Smartphone for 1200 ETB! Buy now!',
        'Laptop available for 5000 ETB.',
        'Shoes for sale, 500 ETB.',
        'T-shirt priced at 300 ETB.'
    ],
    'views': [2000, 1500, 1000, 800],
    'timestamp': ['2025-06-01 12:00:00', '2025-06-08 14:00:00', '2025-06-05 10:00:00', '2025-06-12 16:00:00']
}
sample_df = pd.DataFrame(sample_data)
sample_df['timestamp'] = pd.to_datetime(sample_df['timestamp'])

# Check if df has required columns; otherwise, use sample data
print("Available columns in DataFrame:", df.columns.tolist())
text_column = 'content'  # Replace with actual text column (e.g., 'message')
required_columns = ['vendor', 'views', 'timestamp', text_column]

if not all(col in df.columns for col in required_columns):
    print(f"Warning: Missing required columns {required_columns}. Using sample DataFrame.")
    df = sample_df
    print("Sample DataFrame columns:", df.columns.tolist())
else:
    # Rename columns if they have different names
    column_mapping = {
        'vendor': 'vendor',  # Update if named 'channel', 'user', etc.
        'views': 'views',    # Update if named 'view_count', 'impressions'
        'timestamp': 'timestamp',  # Update if named 'date', 'posted_at'
        text_column: text_column   # Update if named 'message', 'text'
    }
    df = df.rename(columns={v: k for k, v in column_mapping.items()})

# Resolve model path
model_path = os.path.abspath("fine_tuned_xlm-roberta-base")
fallback_model = "xlm-roberta-base"

# Load NER model and tokenizer
try:
    if not os.path.exists(model_path):
        print(f"Local model directory '{model_path}' not found. Using fallback model '{fallback_model}'.")
        tokenizer = AutoTokenizer.from_pretrained(fallback_model)
        model = AutoModelForTokenClassification.from_pretrained(fallback_model, num_labels=9)
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForTokenClassification.from_pretrained(model_path)
except Exception as e:
    print(f"Error loading model from {model_path}: {e}. Falling back to '{fallback_model}'.")
    tokenizer = AutoTokenizer.from_pretrained(fallback_model)
    model = AutoModelForTokenClassification.from_pretrained(fallback_model, num_labels=9)

# Define NER labels
label_list = ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-PRICE', 'I-PRICE', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']
id_to_label = {i: label for i, label in enumerate(label_list)}

# Initialize NER pipeline
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# Clean DataFrame
df = df.dropna(subset=[text_column, 'views', 'timestamp', 'vendor'])
df = df[df[text_column].str.strip() != '']

# Convert timestamps to datetime
try:
    df['timestamp'] = pd.to_datetime(df['timestamp'])
except ValueError as e:
    raise ValueError(f"Invalid timestamp format: {e}. Ensure 'timestamp' is in a parseable format (e.g., '2025-06-01 12:00:00').")

# Extract entities if not pre-extracted
def extract_entities(row):
    if 'tokens' in row and 'ner_tags' in row and isinstance(row['tokens'], list) and isinstance(row['ner_tags'], list):
        tokens = row['tokens']
        ner_tags = row['ner_tags']
        products = []
        prices = []
        current_product = []
        current_price = []
        for token, tag in zip(tokens, ner_tags):
            if tag == 'B-PRODUCT':
                if current_product:
                    products.append(' '.join(current_product))
                current_product = [token]
            elif tag == 'I-PRODUCT':
                current_product.append(token)
            elif tag == 'B-PRICE':
                if current_price:
                    prices.append(' '.join(current_price))
                current_price = [token]
            elif tag == 'I-PRICE':
                current_price.append(token)
        if current_product:
            products.append(' '.join(current_product))
        if current_price:
            prices.append(' '.join(current_price))
        return products, prices
    else:
        text = row[text_column]
        if not isinstance(text, str):
            return [], []
        tokens = word_tokenize(text)
        if not tokens:
            return [], []
        ner_results = ner_pipeline(text)
        products = []
        prices = []
        current_product = []
        current_price = []
        for entity in ner_results:
            word = entity['word']
            label = entity['entity_group']
            if label.startswith('B-PRODUCT'):
                if current_product:
                    products.append(' '.join(current_product))
                    current_product = []
                current_product.append(word)
            elif label.startswith('I-PRODUCT'):
                current_product.append(word)
            elif label.startswith('B-PRICE'):
                if current_price:
                    prices.append(' '.join(current_price))
                    current_price = []
                current_price.append(word)
            elif label.startswith('I-PRICE'):
                current_price.append(word)
        if current_product:
            products.append(' '.join(current_product))
        if current_price:
            prices.append(' '.join(current_price))
        return products, prices

# Apply NER if 'products' and 'prices' columns don't exist
if 'products' not in df.columns or 'prices' not in df.columns:
    df[['products', 'prices']] = pd.DataFrame(
        df.apply(extract_entities, axis=1).tolist(),
        index=df.index
    )

# Parse prices to numeric (assuming ETB)
def parse_price(price_str):
    if not price_str:
        return np.nan
    # Extract numeric value (e.g., "1000 ETB" -> 1000)
    match = re.search(r'[\d,]+(?:\.\d+)?', price_str.replace(',', ''))
    if match:
        try:
            return float(match.group().replace(',', ''))
        except ValueError:
            return np.nan
    return np.nan

df['price_numeric'] = df['prices'].apply(lambda x: parse_price(' '.join(x)) if x else np.nan)

# Calculate vendor metrics
vendor_metrics = []
min_date = df['timestamp'].min()
max_date = df['timestamp'].max()
weeks = (max_date - min_date).days / 7 if (max_date - min_date).days > 0 else 1

for vendor, group in df.groupby('vendor'):
    # Posting Frequency (posts/week)
    post_count = len(group)
    posting_frequency = post_count / weeks

    # Average Views per Post
    avg_views = group['views'].mean()

    # Top Performing Post
    top_post = group.loc[group['views'].idxmax()]
    top_post_views = top_post['views']
    top_post_product = ' '.join(top_post['products']) if top_post['products'] else 'Unknown'
    top_post_price = top_post['price_numeric'] if not pd.isna(top_post['price_numeric']) else 'Unknown'
    top_post_text = top_post[text_column][:100] + '...' if len(top_post[text_column]) > 100 else top_post[text_column]

    # Average Price Point
    avg_price = group['price_numeric'].mean() if not group['price_numeric'].isna().all() else np.nan

    # Lending Score (normalize metrics)
    views_norm = (avg_views - df['views'].min()) / (df['views'].max() - df['views'].min()) if df['views'].max() != df['views'].min() else 0
    freq_norm = (posting_frequency - df.groupby('vendor').size().div(weeks).min()) / (df.groupby('vendor').size().div(weeks).max() - df.groupby('vendor').size().div(weeks).min()) if df.groupby('vendor').size().div(weeks).max() != df.groupby('vendor').size().div(weeks).min() else 0
    price_norm = (avg_price - df['price_numeric'].min()) / (df['price_numeric'].max() - df['price_numeric'].min()) if not df['price_numeric'].isna().all() and df['price_numeric'].max() != df['price_numeric'].min() else 0
    lending_score = (0.4 * views_norm) + (0.3 * freq_norm) + (0.3 * price_norm)

    vendor_metrics.append({
        'Vendor': vendor,
        'Posts/Week': posting_frequency,
        'Avg. Views/Post': avg_views,
        'Top Post Views': top_post_views,
        'Top Post Product': top_post_product,
        'Top Post Price (ETB)': top_post_price,
        'Top Post Text': top_post_text,
        'Avg. Price (ETB)': avg_price,
        'Lending Score': lending_score
    })

# Create vendor metrics DataFrame
vendor_df = pd.DataFrame(vendor_metrics)

# Generate report
report_lines = []
report_lines.append("# Vendor Scorecard Report\n")
report_lines.append("## Date: June 23, 2025\n")
report_lines.append("### Summary\n")
report_lines.append("This report evaluates EthioMart vendors for micro-lending potential based on Telegram post activity and NER-extracted entities.\n")

report_lines.append("\n## Vendor Scorecard\n")
scorecard_table = vendor_df[['Vendor', 'Avg. Views/Post', 'Posts/Week', 'Avg. Price (ETB)', 'Lending Score']].copy()
scorecard_table['Avg. Views/Post'] = scorecard_table['Avg. Views/Post'].round(2)
scorecard_table['Posts/Week'] = scorecard_table['Posts/Week'].round(2)
scorecard_table['Avg. Price (ETB)'] = scorecard_table['Avg. Price (ETB)'].round(2)
scorecard_table['Lending Score'] = scorecard_table['Lending Score'].round(2)
report_lines.append(scorecard_table.to_markdown(index=False))
report_lines.append("\n")

report_lines.append("## Detailed Metrics\n")
for _, row in vendor_df.iterrows():
    report_lines.append(f"### Vendor: {row['Vendor']}\n")
    report_lines.append(f"- **Posts/Week**: {row['Posts/Week']:.2f}\n")
    report_lines.append(f"- **Avg. Views/Post**: {row['Avg. Views/Post']:.2f}\n")
    report_lines.append(f"- **Avg. Price (ETB)**: {row['Avg. Price (ETB)']:.2f}\n")
    report_lines.append(f"- **Lending Score**: {row['Lending Score']:.2f}\n")
    report_lines.append(f"- **Top Performing Post**:\n")
    report_lines.append(f"  - Views: {row['Top Post Views']}\n")
    report_lines.append(f"  - Product: {row['Top Post Product']}\n")
    report_lines.append(f"  - Price (ETB): {row['Top Post Price (ETB)']}\n")
    report_lines.append(f"  - Text: {row['Top Post Text']}\n")

report_lines.append("\n## Methodology\n")
report_lines.append("- **Posting Frequency**: Calculated as total posts divided by weeks in the data period.\n")
report_lines.append("- **Average Views**: Mean views per post.\n")
report_lines.append("- **Top Post**: Post with highest views, with product and price extracted via NER.\n")
report_lines.append("- **Average Price**: Mean of extracted prices (in ETB).\n")
report_lines.append("- **Lending Score**: Weighted sum of normalized metrics (0.4 * Avg Views + 0.3 * Posts/Week + 0.3 * Avg Price).\n")

report_lines.append("\n## Recommendations\n")
report_lines.append("- **High-Scoring Vendors**: Prioritize vendors with Lending Scores > 0.7 for micro-lending.\n")
report_lines.append("- **Improve Data Quality**: Ensure consistent price formats and timestamp accuracy.\n")
report_lines.append("- **Enhance NER**: Fine-tune model on domain-specific data to improve product/price extraction.\n")

# Save report
with open("vendor_scorecard_report.md", "w", encoding="utf-8") as f:
    f.write("".join(report_lines))

# Save vendor metrics to CSV
vendor_df.to_csv("vendor_metrics.csv", index=False)

print("Vendor Scorecard report saved to 'vendor_scorecard_report.md'")
print("Vendor metrics saved to 'vendor_metrics.csv'")